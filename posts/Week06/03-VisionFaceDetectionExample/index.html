<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"/><meta property="og:site_name" content="Mobile Application Development for the iOS Platform"/><link rel="canonical" href="https://hococoder.com/posts/Week06/03-VisionFaceDetectionExample"/><meta name="twitter:url" content="https://hococoder.com/posts/Week06/03-VisionFaceDetectionExample"/><meta property="og:url" content="https://hococoder.com/posts/Week06/03-VisionFaceDetectionExample"/><title>Week 6 - Vision Face Detection Example | Mobile Application Development for the iOS Platform</title><meta name="twitter:title" content="Week 6 - Vision Face Detection Example | Mobile Application Development for the iOS Platform"/><meta property="og:title" content="Week 6 - Vision Face Detection Example | Mobile Application Development for the iOS Platform"/><meta name="description" content="Engineering Program for Professionals - 605.687 - 2025-2026 Academic Year"/><meta name="twitter:description" content="Engineering Program for Professionals - 605.687 - 2025-2026 Academic Year"/><meta property="og:description" content="Engineering Program for Professionals - 605.687 - 2025-2026 Academic Year"/><meta name="twitter:card" content="summary"/><link rel="stylesheet" href="../../../styles.css" type="text/css"/><script src="../../../../jquery-3.7.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1.0"/><link rel="shortcut icon" href="/images/favicon.png" type="image/png"/><link rel="alternate" href="/feed.rss" type="application/rss+xml" title="Subscribe to Mobile Application Development for the iOS Platform"/></head><body><div class="layout"><div class="header"><div class="headerLayout"><div class="title">Mobile Application Development for the iOS Platform</div><div class="subtitle">Engineering Program for Professionals - 605.687 - 2025-2026 Academic Year</div><div class="weeklayout"><a class="grow1" href="/epp_605_687_spring_2026/posts/Intro/About Me">About Me</a><a class="grow1" href="/epp_605_687_spring_2026/posts/Intro/Example App">Example App</a><a class="grow1" href="/epp_605_687_spring_2026//posts//Week00//00-Week00Overview">Week 0</a><a class="grow1" href="/epp_605_687_spring_2026/posts/Week01/00-Week01Overview">Week 1</a><a class="grow1" href="/epp_605_687_spring_2026/posts/Week02/00-Week02Overview">Week 2</a><a class="grow1" href="/epp_605_687_spring_2026/posts/Week03/00-Week03Overview">Week 3</a><a class="grow1" href="/epp_605_687_spring_2026/posts/Week04/00-Week04Overview">Week 4</a><a class="grow1" href="/epp_605_687_spring_2026/posts/Week05/00-Week05Overview">Week 5</a><a class="grow1" href="/epp_605_687_spring_2026/posts/Week06/00-Week06Overview">Week 6</a><a class="grow1" href="/epp_605_687_spring_2026/posts/Week07/00-Week07Overview">Week 7</a><a class="grow1" href="/epp_605_687_spring_2026/posts/Week08/00-Week08Overview">Week 8</a><a class="grow1" href="/epp_605_687_spring_2026/posts/Week09/00-Week09Overview">Week 9</a><a class="grow1" href="/epp_605_687_spring_2026/posts/Week10/00-Week10Overview">Week 10</a><a class="grow1" href="/epp_605_687_spring_2026/posts/Week11/00-Week11Overview">Week 11</a><a class="grow1" href="/epp_605_687_spring_2026/posts/Week12/00-Week12Overview">Week 12</a><a class="grow1" href="/epp_605_687_spring_2026/posts/Week13/00-Week13Overview">Week 13</a></div></div></div><div class="wrapper"><h1>Week 6 - Vision Face Detection Example</h1><p style="float: left">Previous:  <A HREF="../../Week06/02-CoreImageFiltersExample/index.html">CoreImage Filters Example</A></p><p style="float: right">Next:  <A HREF="../../Week06/04-MakingCustomControlsWithSwiftUI/index.html">Making Custom Controls With SwiftUI</A></p><BR/><BR/><center><iframe id="myIframe" src="https://jh.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=3c5c84d6-dee4-453c-8a18-b1ba014e8a4c&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all"  style="border: 1px solid #464646;" allowfullscreen allow="autoplay" aria-label="Panopto Embedded Video Player" aria-description="Week0-04-THIRDPARTYTOOLS-Lecture" ></iframe></center><p>Let's extend on the last example, and add functionality to detect faces. Here, we'll be using the <code>Vision</code> framework, which has routines specifically designed to look for features in an image, such as facial features, barcodes, and more.</p><p>//TODO confirm this is still bad on iOS 26</p><p class="info"> There is an issue in iOS 15 and above where this will NOT WORK ON THE SIMULATOR.  You must run this on a device</p><p>Start by making a new view, <code>FaceDetectionView</code>, and declaring some properties:</p><pre><code><span class="keyword">struct</span> FaceDetectionView: <span class="type">View</span> {

  <span class="keyword">@State var</span> image: <span class="type">Image</span>?
  <span class="keyword">@State var</span> uiImage: <span class="type">UIImage</span>!
  <span class="keyword">@ObservedObject var</span> faceDetector: <span class="type">FaceDetector</span> = <span class="type">FaceDetector</span>()

  <span class="keyword">init</span>() {
    faceDetector = <span class="type">FaceDetector</span>()
  }
</code></pre><p>As before, we have an <code>Image</code> and a <code>UIImage</code>, because the libraries we'll call need the various forms. Next there is a <code>FaceDetector</code> property which we will define shortly. It is initialized in the <code>init()</code> method.</p><p>Next, start to define the body of the view.</p><pre><code>  <span class="keyword">var</span> body: <span class="keyword">some</span> <span class="type">View</span> {
    <span class="type">ZStack</span> {
      image
      <span class="type">Canvas</span> { context, size <span class="keyword">in</span>

      }
      .<span class="call">frame</span>(width: <span class="number">382</span>, height: <span class="number">436</span>)
    }
    .<span class="call">onAppear</span>(perform: loadImage)
</code></pre><p>The <code>body</code> has a <code>ZStack</code>, in which lies the <code>image</code> as well as a <code>Canvas</code> view. <code>Canvas</code> is new in iOS 15, and uses Core Graphics to render graphics into the view. We'll dive into Core Graphics in a few modules. For now, the <code>Canvas</code> closure is empty, and it is set to the same size as the image. When the <code>ZStack</code> loads, the <code>image</code> is loaded, just like last module.</p><pre><code>  <span class="keyword">func</span> loadImage() {

    <span class="keyword">guard let</span> uiImage = <span class="type">UIImage</span>(named: <span class="string">"cook_hero_small"</span>) <span class="keyword">else</span> { <span class="keyword">return</span> }
    <span class="keyword">self</span>.<span class="property">uiImage</span> = uiImage
    image = <span class="type">Image</span>(uiImage: uiImage)
    faceDetector.<span class="property">image</span> = uiImage.<span class="property">cgImage</span>!
    faceDetector.<span class="call">performVisionRequest</span>(image: uiImage.<span class="property">cgImage</span>!, orientation: .<span class="dotAccess">up</span>)
  }
</code></pre><p>The one addition here is that the image of the <code>faceDetector</code> is set, and then we ask the detector to <code>performVisionRequest</code>, using the the core graphics version of the image (created with <code>.cgImage</code>), and specifying the orientation of the image is <code>.up</code>.</p><p>OK, now to the meat of the code: the <code>FaceDetector</code>. This will be an <code>ObservableObject</code>, with 2 published properties: an array of <code>boundingBoxes</code> for the general face detection, and an array of <code>closedRegions</code> for the facial features. We'll use 2 types of Vision requests for this: <code>VNDetectFaceRectanglesRequest</code>, and <code>VNDetectFaceLandmarksRequest</code>:</p><pre><code><span class="keyword">class</span> FaceDetector: <span class="type">ObservableObject</span> {

  <span class="keyword">@Published var</span> boundingBoxes: [<span class="type">CGRect</span>] = []
  <span class="keyword">@Published var</span> closedRegions: [[<span class="type">CGPoint</span>]] = []
  <span class="keyword">var</span> image: <span class="type">CGImage</span>!
  <span class="keyword">var</span> requests: [<span class="type">VNRequest</span>] = []

  <span class="keyword">lazy var</span> faceDetectionRequest = <span class="type">VNDetectFaceRectanglesRequest</span>(completionHandler: <span class="keyword">self</span>.<span class="property">handleDetectedFaces</span>)
  <span class="keyword">lazy var</span> faceLandmarkRequest = <span class="type">VNDetectFaceLandmarksRequest</span>(completionHandler: <span class="keyword">self</span>.<span class="property">handleDetectedFaceLandmarks</span>)


  <span class="keyword">init</span>() {
    requests.<span class="call">append</span>(faceDetectionRequest)
    requests.<span class="call">append</span>(faceLandmarkRequest)
  }
</code></pre><p>The handlers for the requests will be defined shortly.</p><p>Next, define the <code>performVisionRequest</code> method we called from the view:</p><pre><code><span class="keyword">func</span> performVisionRequest(image: <span class="type">CGImage</span>, orientation: <span class="type">CGImagePropertyOrientation</span>) {

      <span class="comment">// Create a request handler.</span>
      <span class="keyword">let</span> imageRequestHandler = 
      <span class="type">VNImageRequestHandler</span>(cgImage: image, 
                            orientation: orientation, 
                            options: [:])

      <span class="comment">// Send the requests to the request handler.</span>
      <span class="type">DispatchQueue</span>.<span class="call">global</span>(qos: .<span class="dotAccess">userInitiated</span>).<span class="call">async</span> {
          <span class="keyword">do</span> {
            <span class="keyword">try</span> imageRequestHandler.<span class="call">perform</span>(<span class="keyword">self</span>.<span class="property">requests</span>)
          } <span class="keyword">catch let</span> error <span class="keyword">as</span> <span class="type">NSError</span> {
              <span class="call">print</span>(<span class="string">"Failed to perform image request:</span> \(error)<span class="string">"</span>)
              <span class="keyword">return</span>
          }
      }
  }
</code></pre><p>This code creates an <code>imageRequestHandler</code> from the image and orientation, and dispatches to a queue the attempt to perform our <code>requests</code> on that handler. Those requests are dispatched to a queue because they are asynchronous - it takes some undetermined amount of time to analyze the image and pass results back to the main program.</p><p>Let's quickly define the 2 handler methods - they are fairly similar, just calling different methods in the end:</p><pre><code>   <span class="keyword">func</span> handleDetectedFaces(request: <span class="type">VNRequest</span>?, error: <span class="type">Error</span>?) {
    <span class="keyword">if let</span> nsError = error <span class="keyword">as</span> <span class="type">NSError</span>? {
      <span class="call">print</span>(<span class="string">"Error detecting faces</span> \(nsError)<span class="string">"</span>)
      <span class="keyword">return</span>
    }
    <span class="comment">// Perform drawing on the main thread.</span>
    <span class="type">DispatchQueue</span>.<span class="property">main</span>.<span class="call">async</span> {
      <span class="keyword">guard let</span> results = request?.<span class="property">results</span> <span class="keyword">as</span>? [<span class="type">VNFaceObservation</span>] <span class="keyword">else</span> {
        <span class="keyword">return</span>
      }
      <span class="keyword">self</span>.<span class="call">draw</span>(faces: results)
    }
  }

  <span class="keyword">func</span> handleDetectedFaceLandmarks(request: <span class="type">VNRequest</span>?, error: <span class="type">Error</span>?) {
    <span class="keyword">if let</span> nsError = error <span class="keyword">as</span> <span class="type">NSError</span>? {
      <span class="call">print</span>(<span class="string">"Error detecting facial features</span> \(nsError)<span class="string">"</span>)
      <span class="keyword">return</span>
    }
    <span class="comment">// Perform drawing on the main thread.</span>
    <span class="type">DispatchQueue</span>.<span class="property">main</span>.<span class="call">async</span> {
      <span class="keyword">guard let</span> results = request?.<span class="property">results</span> <span class="keyword">as</span>? [<span class="type">VNFaceObservation</span>] <span class="keyword">else</span> {
        <span class="keyword">return</span>
      }
      <span class="keyword">self</span>.<span class="call">drawFeatures</span>(faces: results)
    }
  }
</code></pre><p>The main difference between these 2 methods is what is called at the end - either <code>draw</code> for the face boxes, or <code>drawFeatures</code> for the facial features. Let's see how those differ, starting with <code>draw</code>:</p><pre><code>   <span class="keyword">func</span> draw(faces: [<span class="type">VNFaceObservation</span>]) {
    <span class="keyword">for</span> observation <span class="keyword">in</span> faces {
      <span class="keyword">let</span> rect = <span class="call">convertFaceBoxToImageCoords</span>(box: observation.<span class="property">boundingBox</span>)  <span class="comment">//1</span>
      <span class="keyword">self</span>.<span class="property">boundingBoxes</span>.<span class="call">append</span>(rect) <span class="comment">//2</span>
    }
  }

  <span class="keyword">func</span> convertFaceBoxToImageCoords(box: <span class="type">CGRect</span>) -&gt; <span class="type">CGRect</span> {
    <span class="keyword">let</span> imageWidth = <span class="number">382.0</span>
    <span class="keyword">let</span> imageHeight = <span class="number">436.0</span>

    <span class="comment">// Begin with input rect.</span>
    <span class="keyword">var</span> rect = box

    <span class="comment">// Rescale normalized coordinates.  //3</span>
    rect.<span class="property">size</span>.<span class="property">width</span> *= imageWidth
    rect.<span class="property">size</span>.<span class="property">height</span> *= imageHeight

    <span class="comment">// Reposition origin.       //4</span>
    rect.<span class="property">origin</span>.<span class="property">x</span> *= imageWidth
    rect.<span class="property">origin</span>.<span class="property">y</span> = rect.<span class="property">origin</span>.<span class="property">y</span>*imageHeight - rect.<span class="property">size</span>.<span class="property">height</span>/<span class="number">2</span> + <span class="number">15</span>
    <span class="keyword">return</span> rect;
  }
</code></pre><p>Here:</p><ol><li>This call converts the bounding box to image coordinates, so we can eventually draw it on the Canvas. Core Graphics and many other coordinate systems are (still! after all these years!) flipped with each other in the y direction (the canvas origin is upper left, Core Graphics is lower right).</li><li>Once we have the converted <code>CGRect</code> object, store it in the array of <code>boundingBoxes</code>.</li><li>The coordinates of the original <code>boundingBox</code> are normalized (from 0 to 1) to we need to multiply by the image width and height (note that I've hardcoded the image dimensions just for convenience).</li><li>The origin of the rect also needs to be scaled, but it also needs to be flipped in the y-direction. This is a little convoluted though. When we switch frames from the Core Graphics to the image frame we need to also offset in the y direction by <code>rect.size.height/2</code>. I've also added in a small offset of 15 to line up the box a little better to the contour of the face.</li></ol><p>The facial features are a bit different - here we are collecting points to define paths that we'll draw later.</p><pre><code>   <span class="keyword">func</span> drawFeatures(faces: [<span class="type">VNFaceObservation</span>]) {
    <span class="keyword">for</span> observation <span class="keyword">in</span> faces {
      <span class="keyword">let</span> faceBoundingBox = <span class="call">convertFaceBoxToImageCoords</span>(box: observation.<span class="property">boundingBox</span>)
      <span class="keyword">guard let</span> landmarks = observation.<span class="property">landmarks</span> <span class="keyword">else</span> {  <span class="comment">//1</span>
          <span class="keyword">continue</span>
      }
      <span class="keyword">let</span> closedLandmarkRegions = [ <span class="comment">//2</span>
          landmarks.<span class="property">leftEye</span>,
          landmarks.<span class="property">rightEye</span>,
          landmarks.<span class="property">outerLips</span>,
          landmarks.<span class="property">innerLips</span>,
          landmarks.<span class="property">nose</span>
          ].<span class="call">compactMap</span> { $0 } <span class="comment">// Filter out missing regions.</span>

      <span class="keyword">for</span> closedLandmarkRegion <span class="keyword">in</span> closedLandmarkRegions { <span class="comment">//3</span>
        <span class="keyword">var</span> scaledPoints: [<span class="type">CGPoint</span>] = []
        <span class="keyword">for</span> point <span class="keyword">in</span> closedLandmarkRegion.<span class="property">normalizedPoints</span> {
          <span class="keyword">let</span> pt = <span class="type">CGPoint</span>(x: faceBoundingBox.<span class="property">origin</span>.<span class="property">x</span> +  point.<span class="property">x</span>*faceBoundingBox.<span class="property">width</span>, y: (faceBoundingBox.<span class="property">origin</span>.<span class="property">y</span> + (<span class="number">1</span>-point.<span class="property">y</span>)*faceBoundingBox.<span class="property">height</span> + <span class="number">15</span>))
          scaledPoints.<span class="call">append</span>(pt)
        }

        closedRegions.<span class="call">append</span>(scaledPoints)
      }
    }
  }
</code></pre><p>Here, we:</p><ol><li>Grab the landmarks for this observation.</li><li>Define which regions we want to search on; if one of them is null, <code>compactMap</code> removes it.</li><li>For each region, scale the normalized points - which are from 0 to 1 <em>within the face bounding box</em> to the image coordinates. Then append it to the <code>closedRegions</code> array.</li></ol><p>With our arrays now populated, let's go back and fill out the closure for the <code>Canvas</code>:</p><pre><code><span class="type">Canvas</span> { context, size <span class="keyword">in</span>

       context.<span class="call">withCGContext</span> { cgContext <span class="keyword">in
         for</span> rect <span class="keyword">in</span> faceDetector.<span class="property">boundingBoxes</span> {
           cgContext.<span class="call">setFillColor</span>(<span class="type">UIColor</span>.<span class="property">clear</span>.<span class="property">cgColor</span>)
           cgContext.<span class="call">setStrokeColor</span>(<span class="type">UIColor</span>.<span class="property">yellow</span>.<span class="property">cgColor</span>)
           cgContext.<span class="call">addRect</span>(rect)
           cgContext.<span class="call">drawPath</span>(using: .<span class="dotAccess">fillStroke</span>)
         }
         <span class="keyword">for</span> region <span class="keyword">in</span> faceDetector.<span class="property">closedRegions</span> {
           cgContext.<span class="call">addLines</span>(between: region)
           cgContext.<span class="call">strokePath</span>()
         }
       }
}
</code></pre><p>Here, we are using some Core Graphics calls to take each face bounding box, add it and draw it with a yellow color (and a clear fill). For the facial feature regions, use the array of <code>CGPoint</code>s from that to <code>addLines</code> between each point, and stroke the path.</p><p>//TODO Confirm this is still the case</p><p class="info"> There is an issue in iOS 15 where this will NOT WORK ON THE SIMULATOR.  You must run this on a device</p><p>On a device, it looks like this:</p><img src="../../../week06/FacialFeatures.jpeg#centerResized" alt="Facial Features"/><p>Vision uses machine learning to identify features, so it is bound to get better and better over time!</p><p style="float: left">Previous:  <A HREF="../../Week06/02-CoreImageFiltersExample/index.html">CoreImage Filters Example</A></p><p style="float: right">Next:  <A HREF="../../Week06/04-MakingCustomControlsWithSwiftUI/index.html">Making Custom Controls With SwiftUI</A></p><BR/><BR/></div><leftside><u><p>Week 6</p></u><div class="weekSection"><ul><li><div class="comment-container"><div class="tocItem"><a id="Week 06 Overview" href="../../Week06/00-Week06Overview/index.html">Week 06 Overview</a></div><div class="chain chain-bottom"></div></div></li><li><div class="comment-container"><div class="chain chain-top"></div><div class="tocItem"><a id="Introduction to CoreImage" href="../../Week06/01-IntroductionToCoreImage/index.html">Introduction to CoreImage</a></div><div class="chain chain-bottom"></div></div></li><li><div class="comment-container"><div class="chain chain-top"></div><div class="tocItem"><a id="CoreImage Filters Example" href="../../Week06/02-CoreImageFiltersExample/index.html">CoreImage Filters Example</a></div><div class="chain chain-bottom"></div></div></li><li><div class="comment-container"><div class="chain chain-top"></div><div class="tocItem"><a id="Vision Face Detection Example" href="../../Week06/03-VisionFaceDetectionExample/index.html">Vision Face Detection Example</a></div><div class="chain chain-bottom"></div></div></li><li><div class="comment-container"><div class="chain chain-top"></div><div class="tocItem"><a id="Making Custom Controls With SwiftUI" href="../../Week06/04-MakingCustomControlsWithSwiftUI/index.html">Making Custom Controls With SwiftUI</a></div><div class="chain chain-bottom"></div></div></li><li><div class="comment-container"><div class="chain chain-top"></div><div class="tocItem"><a id="Making Custom Controls With CoreGraphics" href="../../Week06/05-MakingCustomControlsWithCoreGraphics/index.html">Making Custom Controls With CoreGraphics</a></div><div class="chain chain-bottom"></div></div></li><li><div class="comment-container"><div class="chain chain-top"></div><div class="tocItem"><a id="Introduction to SwiftUI Charts" href="../../Week06/06-IntroductionToSwiftUICharts/index.html">Introduction to SwiftUI Charts</a></div></div></li></ul></div></leftside><footer><p>Generated with ❤️ using <a href="https://github.com/johnsundell/publish">Publish</a></p><p>© 2025-2026 Josh Steele (rsteele3@jhu.edu) <a href="mailto:"(rsteele3@jhu.edu)""></a></p><p>Last updated January 19, 2026 at 3:52 PM</p><p><a href="https://mastodon.social/@hococoder" target="_blank">My Mastodon</a> | <a href="https://github.com/hococoder" target="_blank">My GitHub</a> | <a href="https://ep.jhu.edu" target="_blank">Whiting School EPP</a> | <a href="https://canvas.jhu.edu" target="_blank">JHU Canvas</a></p></footer></div></body></html>